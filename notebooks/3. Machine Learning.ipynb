{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/APSV-UPM/BusinessIntelligence/main/data/data_clean.csv > data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "Machine Learning is the study and application of algorithms that generate statistic models that are able to perform a specific task without being specifically coded. Models are created by observing a set of samples and extracting and learning the patterns in the samples data.\n",
    "\n",
    "The most used library in Python to work with ML is SciKit-Learn https://scikit-learn.org/stable/. This module includes the implementations of dozens of different ML algorithms and the functions needed to handle and transform the data that will be used by the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our well known dataset of train events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_clean.csv\").dropna()\n",
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to transform this train event data set into a table in which each row contains the information for a single train trip. These rows will indicate the number of packages that were transported on the train, the train used and the trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby(\"trip_id\").agg({\"train_id\":\"first\", \"date\":{'min', 'max'}, \"cargo\":'max'})\n",
    "t.columns.map(\"_\".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_summary = df.groupby(\"trip_id\").agg({\"train_id\":\"first\", \"date\":{'min', 'max'}, \"cargo\":'max'})\n",
    "trip_summary.columns = trip_summary.columns.map(\"_\".join)\n",
    "trip_summary.columns = trip_summary.rename(columns={\"date_min\":\"start_date\", \"date_max\":\"end_date\", \"train_id_first\":\"train_id\", \"cargo_max\":\"cargo\"}).columns\n",
    "trip_summary.reset_index(inplace=True, drop=True)\n",
    "trip_summary[\"trip_duration\"] = (trip_summary.end_date - trip_summary.start_date).dt.total_seconds()\n",
    "trip_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ML algorithms can only work with numerical values, therefore, we have to convert the columns that contain text (categorical variables) into numbers. The simplest way to do this is by using a LabelEncoder, which just assigns each category to which it finds an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "train_id_encoder = preprocessing.LabelEncoder().fit(trip_summary.train_id.unique())\n",
    "\n",
    "for i in range(len(train_id_encoder.classes_)):\n",
    "    print(f\"\"\"{train_id_encoder.classes_[i]} encoded as {i}\"\"\")\n",
    "trip_summary[\"train_id_encoded\"] = pd.Series(train_id_encoder.transform(trip_summary.train_id.values), index = trip_summary.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test\n",
    "The main idea behind machine learning is that the algorithm will observe a training set of samples, which we should be sure to be representative of the data with which we are working with which. From this training set or training data, the algorithm will learn the patterns that describe the data and generates a model which can replicate the behavior shown on them. Then, using a *different* set of samples, which we call test set or test data, we will evaluate how accurate is the generated model, basically, by measuring the difference between the model output and the correct answer. It is very important to do not use the same data to train and test the model.\n",
    "\n",
    "Letâ€™s see it with an example.\n",
    "We will use a popular dataset in ML called iris dataset. This dataset contains a series of measures of flowers and the type of the flower (for more info, check https://archive.ics.uci.edu/ml/datasets/iris). As we will see later, the task of the model we are going to create is a classification task. \n",
    "The dataset is made up of observations that contain 4 numeric variables and 1 categorical target. The task of the model is to predict the target of a sample by observing its numeric variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is based on one example from the scikit learn documentation \n",
    "# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "x_feature = 0\n",
    "y_feature = 1\n",
    "\n",
    "n_neighbors = 5\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# In this notebook, we will always use x as the feature vector and y as the target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.3)\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(x_train, y_train)\n",
    "y_out = clf.predict(x_test)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "\n",
    "ax.scatter(x_train[:, x_feature], x_train[:, y_feature], c=y_train, cmap=\"plasma\", edgecolors='k')\n",
    "ax.scatter(x_test[:, x_feature], x_test[:, y_feature], c=y_out, cmap=\"plasma\", marker = \"x\")\n",
    "ax.scatter(x_test[:, x_feature], x_test[:, y_feature], c=y_test, cmap=\"plasma\", alpha=0.3, marker = \"o\", s=200, edgecolors='k')\n",
    "ax.scatter(x_test[y_out != y_test, x_feature], x_test[y_out != y_test, y_feature], facecolors='none', edgecolors='r', s=200)\n",
    "\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = %i)\" % (n_neighbors))\n",
    "plt.show()\n",
    "\n",
    "print(\"Model accuracy: {:.2f}%\".format(np.average(y_out==y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset has 4 dimensions, the plot only shows 2 dimensions, which is easier for humans. The colors in the plot are the different types of flowers. Dots are the training data and cross the test data, the background of each test sample is the correct type, and the sample is surrounded with a red circle if it was misclasified. The accuracy is just the average of the correct classifications made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "Ensure that the partition of the data used to train the model is representative may not be an easy task. For example, in the previous dataset it could be that the data we select to train don't contain samples of one of the class, or may be that the test data were particularly easy to classify.\n",
    "\n",
    "Cross Validation is a group of techniques that solve this problem, all of them are based on the idea of performing several trainings and testings using different partitions each time. The most basic, and most popular, is the K-Fold Cross Validation.\n",
    "In K-Fold, we choose a number k (usually from 5 to 10), then we divide our dataset into k folds. We will train the model k times, in each iteration we will use k-1 folds as the train data and the remaining fold as test data. The evaluation of the final model is calculated as the average of the evaluations for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k = 3\n",
    "kf = KFold(n_splits=k,shuffle=True)\n",
    "accuracies = []\n",
    "i=0\n",
    "for train, test in kf.split(X): # train and test are the indices of the samples that will be used in each set\n",
    "    i+=1\n",
    "    x_train, x_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_out = clf.predict(x_test)\n",
    "\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(9, 8))\n",
    "    ax.scatter(x_train[:, x_feature], x_train[:, y_feature], c=y_train, cmap=\"plasma\", edgecolors='k')\n",
    "    ax.scatter(x_test[:, x_feature], x_test[:, y_feature], c=y_out, cmap=\"plasma\", marker = \"x\")\n",
    "    ax.scatter(x_test[:, x_feature], x_test[:, y_feature], c=y_test, cmap=\"plasma\", alpha=0.3, marker = \"o\", s=200, edgecolors='k')\n",
    "    ax.scatter(x_test[y_out != y_test, x_feature], x_test[y_out != y_test, y_feature], facecolors='none', edgecolors='r', s=200)\n",
    "    ax.set_title(\"3-Class classification (k = %i) iteration %i\" % (n_neighbors,i))\n",
    "    plt.show()\n",
    "    accuracy = np.average(y_out==y_test)*100\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"Iteration {}: Model accuracy: {:.2f}%\".format(i,accuracy))\n",
    "    print(y_test)\n",
    "    print(y_out)\n",
    "print(\"Global accuracy: {:.2f}%\".format(np.average(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Regression is one of the possible tasks for an ML model. The objective is to predict a numeric output based on the input. One example of this could be to predict the number of products that will be sold based on the previous sales or calculate the probability that the stock market will grow tomorrow.\n",
    "\n",
    "\n",
    "The most basic example of this task is to obtain the relation between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import math\n",
    "\n",
    "x, y = datasets.make_regression(n_samples=30, n_features=1, noise=0)\n",
    "# try to modify the relation between x and y or play with the noise value\n",
    "y = y**2\n",
    "#y = np.sin(y)\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in the real world are more complicated, the target (variable we want to predict) usually depends on more variables (features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some popular algorithms\n",
    "There are many different algorithms or models that can be used for regression tasks. Each of them has its advantages or disadvantages for each kind of problem, but to discuss them is far beyond the scope of this course. We will just try them with their basic configuration and keep the one that best fits our case.\n",
    "\n",
    "#### Linear regression\n",
    "This is the most basic regression model. The algorithm will just try to minimize the error in a function like $Y=aX+b$, where $Y$ is the target and $X$ the vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x, y = datasets.make_regression(n_samples=30, n_features=1, noise=10)\n",
    "\n",
    "#y=y**2\n",
    "model = LinearRegression().fit(x,y)\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x,model.predict(x), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "This model is based on a tree structure in which each node represents a decision and each leaf an output. https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "x_test= np.linspace(x.min(),x.max(), 100)[:, np.newaxis]\n",
    "\n",
    "model = DecisionTreeRegressor().fit(x,y)\n",
    "fig,ax = plt.subplots(figsize=(9, 8))\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_test ,model.predict(x_test),\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "This model belongs to a category called ensemble models, because it is made of the aggregation of several models. In the case of random forest, we will have a set of decision trees which will calculate an output, then the output of the random forest will be the average of those outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test ,model.predict(x_test), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors\n",
    "Given an input, the output of this algorithm will be the average of the $k$ nearest points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=5).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test ,model.predict(x_test), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "This is the most basic Artificial Neural Network https://en.wikipedia.org/wiki/Artificial_neural_network. Opposite  to the other models shown, this one is a \"black box\" and we cannot see how the algorithm calculates the output.\n",
    "\n",
    "Don't worry if you get a warning when running this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model = MLPRegressor(max_iter=50000).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test ,model.predict(x_test), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to evaluate a regression model\n",
    "We have trained 5 models, and we can guess that some are better than others, but we need a formal way to evaluate the models. There are several ways to evaluate a regression model, all of them based on the __error between the output of the model and the real answer__ when evaluating the test data, so the lower the error, the better the model. Let see some of them:\n",
    "\n",
    "1. Max error: $max(| y_i - \\hat{y}_i |)$\n",
    "\n",
    "2. Mean absolute error or MAE: $\\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$\n",
    "\n",
    "3. Mean squared error or MSE: $\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Don't worry about the math, scikit learn will do everything for us!\n",
    "\n",
    "Remember about K-Folds Cross Validation when evaluating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_absolute_error, mean_squared_error, max_error\n",
    "models = {\"Linear Regression\":LinearRegression(), \\\n",
    "          \"Decision Tree\": DecisionTreeRegressor(), \\\n",
    "          \"Random Forest\": RandomForestRegressor(n_estimators=100), \\\n",
    "          \"Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5), \\\n",
    "          \"Perceptron\": MLPRegressor(max_iter=2000)}\n",
    "\n",
    "x, y = datasets.make_regression(n_samples=30, n_features=1, noise=10)\n",
    "k = 3\n",
    "kf = KFold(n_splits=k,shuffle=True)\n",
    "for name in models:\n",
    "    print(\"Start training models of {}\".format(name))\n",
    "    i = 0\n",
    "    me_global = []\n",
    "    mae_global = []\n",
    "    mse_global = []\n",
    "    for train, test in kf.split(x):\n",
    "        i+=1\n",
    "        x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "        model = models[name].fit(x_train,y_train)\n",
    "        y_out = model.predict(x_test)\n",
    "        # We have to provide the real value of the target and the model's output\n",
    "        me = max_error(y_test, y_out)\n",
    "        mae = mean_absolute_error(y_test, y_out)\n",
    "        mse = mean_squared_error(y_test, y_out)\n",
    "        me_global.append(me)\n",
    "        mae_global.append(mae)\n",
    "        mse_global.append(mse)\n",
    "        print(\"Iteration {}: me={:.4f} mae={:.4f} mse={:.4f}\".format(i,me, mae, mse))\n",
    "    print(\"Global: me={:.4f} mae={:.4f} mse={:.4f}\\n\".format(np.average(me_global), np.average(mae_global), np.average(mse_global)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it\n",
    "Now is your turn to create a prediction model. Just after reading the datasets, we have transformed and prepared them in a format suitable for the algorithms we have just seen.\n",
    "\n",
    "For this case, lets supose that we know the train that will be used in a trip and how many packages will be transported, and we want to predict the duration of the trip. That means that our features are the train_id and the cargo column, and the target is the duration column.\n",
    "\n",
    "The task is to train some regression models using different algorithms and keep the best one. Then answer the following questions:\n",
    "\n",
    "- What is the best algorithm?\n",
    "- What measure have you used to select the best model?\n",
    "- Do you think that the best model is a \"good\" regression model? Why?\n",
    "- Can you think of a variable that we can add to our features to improve the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n",
    "x = trip_summary[[\"train_id_encoded\", \"cargo\"]].values\n",
    "y = trip_summary.trip_duration.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate this task, you will be asked to run your model in a given situation. \n",
    "# For example, the train JITAR has a scheduled trip on 2021-01-23 at 10:00:00, its cargo will be 50 packages. How long will it take to complete the trip?\n",
    "\n",
    "# Suppose that your model is called \"model\" and your input is a vector of [train_id, cargo]. Then, you will have to run the following code:\n",
    "# model.predict([train_id_encoder.transform([\"JITAR\"])[0],50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Another common task in ML is classification; in this case the target will be a class or a categorical variable, like in the iris dataset.\n",
    "\n",
    "When talking about classifications task there are two possibilities: the target can only be positive or negative (binary classification) or the target can be any class of a set of categories (multi class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_classification(n_samples=300, n_features=2,n_classes=4,n_redundant=0,n_clusters_per_class=1)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some popular models\n",
    "Like in the case of regression there are many algorithms for classification. Many of them have implementations for both regression and classification. \n",
    "\n",
    "In the following plots, the decision regions will be plotted. Those are the regions that the model will use to perform the classification, an input will be classified depending on which region it appears.\n",
    "\n",
    "#### Logistic regression\n",
    "Don't be misleading with the regression word, this model can only be used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x, y = datasets.make_classification(n_samples=300, n_features=2,n_classes=3,n_redundant=0,n_clusters_per_class=1)\n",
    "\n",
    "# For decision regions\n",
    "x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "model = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\").fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier().fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(max_iter=2000).fit(x,y)\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(x[:,0], x[:,1],c=y)\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to evaluate a classification model\n",
    "In this case the output is a class or a label, so we can't calculate the error, instead we will calculate a ratio between the number of times that the output is correct and another count of the outputs, the total number or the number of wrong outputs for example. For example, the most intuitive way to evaluate a classification model is the accuracy : How many times my model guess the correct answer over all the anwsers?. But, even if this can be useful in some cases, in practice, it is not an informative measure.\n",
    "\n",
    "### Confusion matrix\n",
    "This is the main tool for evaluating a classification model https://en.wikipedia.org/wiki/Confusion_matrix. It is defined as:\n",
    "\n",
    "\"By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ but predicted to be in group $j$.\"\n",
    "\n",
    "In other words, it is a matrix where columns represent the true class and rows represent the output class. With it we can know how many samples have been correctly classified and which classes can be confused.\n",
    "\n",
    "\n",
    "In the case of binary classification, the matrix has only 2 columns and 2 rows, and each of the cells has a proper name.\n",
    "\n",
    "There are some metrics obtained from these cells:\n",
    "1. Precision: the ratio between positive predicted and the total of positive samples $\\frac{TP}{TP+FP}$\n",
    "2. Accuracy: the ratio of correctly predicted over the total of samples $\\frac{TP+TN}{TP+FP+TN+FN}$\n",
    "3. Recall: the ratio of correctly predicted over all predicted as positive $\\frac{TP}{TP+FN}$\n",
    "4. F1 score: harmonic mean of precision and recall $\\frac{Precision*Recall}{Precision+Recall}=\\frac{2*TP}{2*TP+FP+FN}$\n",
    "\n",
    "Usually more complex metrics are used, but for this course the F1 score is enough. And again, don't worry, scikit learn includes all these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "models = {\"Logistic Regression\":LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=2000), \\\n",
    "          \"Decision Tree\": DecisionTreeClassifier(), \\\n",
    "          \"Random Forest\": RandomForestClassifier(n_estimators=100), \\\n",
    "          \"Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5), \\\n",
    "          \"Perceptron\": MLPClassifier(max_iter=2000)}\n",
    "\n",
    "x, y = datasets.make_classification(n_samples=300)\n",
    "\n",
    "for name in models:\n",
    "    print(\"Start training models of {}\".format(name))\n",
    "    i = 0\n",
    "    f1_global = []\n",
    "    for train, test in kf.split(x):\n",
    "        i+=1\n",
    "        x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "        model = models[name].fit(x_train,y_train)\n",
    "        y_out = model.predict(x_test)\n",
    "        # We have to provide the real value of the target and the model's output\n",
    "        f1_global.append(f1_score(y_test, y_out, average=\"weighted\", zero_division=0))\n",
    "        print(confusion_matrix(y_test, y_out))\n",
    "        print(classification_report(y_test, y_out))\n",
    "    print(\"Global: f1={:.4f}\\n\".format(np.average(f1_global)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it\n",
    "In this case, lets suppose that the sensor which reads the id of a train when it arrives to the last station is malfunctioning. However, we are still able to know how many packages were transported and the duration of the trip. We want to create a model that, given the number of packages and the duration of the trip, can predict the train id.\n",
    "\n",
    "- What is the best algorithm?\n",
    "- What is the average F1 score for that algorithm?\n",
    "\n",
    "Some algorithms provides a way to estimate the importance of each feature in the classification. This is useful to know which features are more important for the model and which are not. Tree based models (Decision Tree and Random Forest) have this feature, we can access it with the attribute feature_importances_. The higher the value, the more important the feature is. In this case, which is the most important feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate this task, you will be asked to run your model in a given situation.\n",
    "# For example, a train has arrive to the last station. Its cargo was 30 packages. \n",
    "# It started its trip on 2021-01-23 at 10:00:00 and arived to the last station on 2021-01-23 at 18:00:00\n",
    "# What is the train's id?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0de1fe606f85d3f2aa9a903f2b0bf89b43d937b91ee737bd06cf0188ff610aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
